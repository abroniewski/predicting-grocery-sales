{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. GET THE DATA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data from CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv('./data/raw/sales.csv')\n",
    "sku = pd.read_csv('./data/raw/sku.csv')\n",
    "geo_params = pd.read_csv('./data/raw/geo_params.csv')\n",
    "test = pd.read_csv('./data/raw/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. BASIC INSPECTION OF THE DATA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4605985 rows and 6 columns. Let's check if there are multiple records for a same sales ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales[sales.duplicated(subset=['ID'], keep= False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no multiple records for the same sales ID, which means that each sale is associated with only one product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are entries with price 0 and sales 0.001. It may be an error. Let´s also check if there are NaN entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_dates = [sales[\"date\"].min(), sales[\"date\"].max()]\n",
    "learning_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 60 rows and 9 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### geoparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_params.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_params.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert date to same format as *sales*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['date'] = pd.to_datetime(test.date, format='%m/%d/%Y')\n",
    "test['date'] = pd.to_datetime(test[\"date\"].dt.strftime('%Y-%m-%d')).astype(str)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 202737 rows and 6 columns. Let's check if there are duplicated records for a same ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[test.duplicated(subset=['ID'], keep= False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the quantitative columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what dates includes the testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dates = [test[\"date\"].min(), test[\"date\"].max()]\n",
    "testing_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.  DATA CLEANING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the *sales* table here are entries with NaN values in price and sales column. Let's change them to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop records with nulls\n",
    "sales.fillna(0, inplace=True)\n",
    "# Confirm that there aren´t any more nulls\n",
    "sales.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The records went from 4605985 to 778366"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's proceed to make a consolidated table <br>\n",
    "First, lets join *sales* with *geoparams* to associate each row in sales to a city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_info = pd.merge(sales, geo_params, how=\"inner\", on='geoCluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.  DATA EXPLORATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a time series for a specific SKU (choose one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_sample = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's filter by SKU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = sales_info.loc[sales_info[\"SKU\"]==sku_sample]\n",
    "subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many geoCluster (locations) associated for a single SKU, thus, we have to analyze them granularily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = subset['geoCluster'].unique()\n",
    "locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick one to create the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --RANDOMLY--\n",
    "geoc = random.choice(locations)\n",
    "\n",
    "#--PICKING A SPECIFIC GEOCLUSTER--\n",
    "# geoc = \n",
    "\n",
    "# Filter by a random geoCluster from the list\n",
    "subset = subset.loc[subset[\"geoCluster\"]==geoc]\n",
    "# Order by date ascendant\n",
    "subset.sort_values(by=['date'], inplace=True)\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------\n",
    "# GRAPH\n",
    "#----------------------------------------------------------------\n",
    "\n",
    "# size:\n",
    "fig = plt.figure(figsize=(30,6))\n",
    "\n",
    "# title:\n",
    "plt.title('Time Series for Sales', fontsize=20)\n",
    "\n",
    "# x axis:\n",
    "# x values\n",
    "x = subset.index.values.tolist()\n",
    "# x ticks\n",
    "my_xticks = subset['date']\n",
    "plt.xticks(x[::7], my_xticks[::7], rotation='90')\n",
    "# # x label\n",
    "plt.xlabel(\"date\", fontsize=16)\n",
    "\n",
    "# y axis:\n",
    "# y values\n",
    "y = subset['sales']\n",
    "# y label\n",
    "plt.ylabel(\"sales\", fontsize=16)\n",
    "\n",
    "\n",
    "# create plot\n",
    "plt.plot(x, y)\n",
    "# add grids to plot\n",
    "plt.grid(color='black', linestyle='-', linewidth=0.5)\n",
    "# show plot\n",
    "plt.show()\n",
    "\n",
    "#----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5.  DATA FORECASTING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the forecasting procedure for the chosen SKU and geoCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First get the training and testing dates list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dates\n",
    "training_dates = sorted(sales[\"date\"].unique())\n",
    "# testing dates\n",
    "testing_dates = sorted(test[\"date\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the variables/columns to train with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(sales_info)[4:6]\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store them in one dataframe (convert to float to do normalization and not loose any information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_training = subset[cols].astype(float)\n",
    "df_for_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized. Let's normalize as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the dataset\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(df_for_training)\n",
    "df_for_training_scaled = scaler.transform(df_for_training)\n",
    "df_for_training_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As required for LSTM networks, we require to reshape an input data into n_samples x timesteps x n_features.<br>\n",
    "In this example, the n_features is 5. We will make timesteps = 14 (past days data used for training). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Empty lists to be populated using formatted training data\n",
    "trainX = []\n",
    "trainY = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_future = 1   # Number of days we want to look into the future based on the past days.\n",
    "n_past = 14  # Number of past days we want to use to predict the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat input data into a shape: (n_samples x timesteps x n_features)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_past, len(df_for_training_scaled) - n_future +1):\n",
    "    trainX.append(df_for_training_scaled[i - n_past:i, 0:df_for_training.shape[1]])\n",
    "    trainY.append(df_for_training_scaled[i + n_future - 1:i + n_future, 0])\n",
    "\n",
    "trainX, trainY = np.array(trainX), np.array(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('trainX shape == {}.'.format(trainX.shape))\n",
    "print('trainY shape == {}.'.format(trainY.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=True))\n",
    "model.add(LSTM(32, activation='relu', return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(trainY.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c562b95383ff9bf63868188c9759f293f388444027b3aff8bbadff699d56422c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('mundimoto')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
